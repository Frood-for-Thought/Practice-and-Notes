{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipelines: https://huggingface.co/docs/transformers/en/main_classes/pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41b0c82553644b058d457f4e8f8fb3c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a929e25eb7c548aa8f4fa53cfe18898d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25190ceff90e4393a5025c3ff57fa5bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ada6cdd2fab4f6e9c70800e25d82b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9598049521446228}]\n"
     ]
    }
   ],
   "source": [
    "# Sentiment classification is the automated process of identifying and classifying emotions in text as positive sentiment, \n",
    "# negative sentiment, or neutral sentiment based on the opinions expressed within. \n",
    "# It helps determine the nature and extent of feelings conveyed using Natural Language Processing (NLP) \n",
    "# to understand what customers say or feel about your brand, products, and services.\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "res = classifier(\"I've been waiting for a HuggingFace course my whole life.\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentiment label was 'POSITIVE' with a score of 96%.\n",
    "\n",
    "The way \"pipeline\" works is:\n",
    "- It first preprocesses the text, applies a tokenizer to the text.\n",
    "- Then it feeds the preprocessed text to the model, and then applies the model.\n",
    "- Then it does the postprocessing.  This means it will show the extpected result for the type of model used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d88156814d64f6ea7e7ad98802137fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "920d4e1e4eab4a4b9de2bbd97bd051b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "463d7eebc1044b39b4f62fe4e00caa1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df9be974f020420eba91054a54220b37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1f5892598124ce6acd3a29c02777e10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7bfba990e144c3e8025554dac10f900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7a11867c5b54c34a755737ce4eed504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'In this course, we will teach you how to create a great website and create a great website. Then we will create a website, create a website'}, {'generated_text': 'In this course, we will teach you how to manage your time and resources in a more personalized way.\\n\\n\\n\\nThe course will take one'}]\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "\n",
    "res = generator(\n",
    "    \"In this course, we will teach you how to\",\n",
    "    max_length = 30, # Different arguments can be found in documentation.\n",
    "    num_return_sequences = 2\n",
    ")\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this course, we will teach you how to create a great website and create a great website. Then we will create a website, create a website\n",
      "In this course, we will teach you how to manage your time and resources in a more personalized way.\n",
      "\n",
      "\n",
      "\n",
      "The course will take one\n"
     ]
    }
   ],
   "source": [
    "for r in res:\n",
    "    print(r['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero-Shot Classification:\n",
    "https://huggingface.co/tasks/zero-shot-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': 'This is a course about Python list comprehension', 'labels': ['education', 'business', 'politics'], 'scores': [0.9622026681900024, 0.026841334998607635, 0.010956003330647945]}\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\") # \"zero-shot\" meaning it will have no memory and be agnostic with the prompt.\n",
    "res = classifier(\n",
    "    \"This is a course about Python list comprehension\",\n",
    "    candidate_labels = [\"education\", \"politics\", \"business\"] # Can use pipeline to classify sequences into any of the class names provided.\n",
    ")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the provided text: \n",
      "'This is a course about Python list comprehension'\n",
      "The classification scores for each label are:\n",
      "\n",
      "education: 0.9622026681900024\n",
      "business: 0.026841334998607635\n",
      "politics: 0.010956003330647945\n"
     ]
    }
   ],
   "source": [
    "print(f\"Given the provided text: \\n'{res['sequence']}'\\nThe classification scores for each label are:\\n\")\n",
    "for i in range(len(res['labels'])):\n",
    "    print(\"{}: {}\".format(res['labels'][i], res['scores'][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer and Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f2df3b69cba4462aca8a42fb84dacd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ed8c7d096941ed9220620c6fa398e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e466def424334aa8ba7934348fe2e160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9edb2a4384b54c17860ae53e52e2593b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\" # Specify a model name.  This is the default model used for the \"sentiment-analysis\" pipeline.\n",
    "# HuggingFace's transformers library has pre-trained models or tokenizers.  The \".from_pretrained()\" method in HuggingFace's transformers library allows a user to load one.\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "# Like with model, load a pre-trained tokenizer from HuggingFace's transformers library.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSITIVE: With a sentiment score of 0.9598049521446228\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "res = classifier(\"I've been waiting for a HuggingFace course my whole life.\")\n",
    "print(f\"{res[0]['label']}: With a sentiment score of {res[0]['score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is the same as the example above, when model and tokenizer was not specified, and the classes \"AutoTokenizer\" and \"AutoModelForSequenceClassification\" were not specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'AutoTokenizer' identifies the pre-trained model and loads the appropriate tokenizer class designed to work with that model, (e.g. BERT, GPT-4, etc.).  It then automatically downloads and caches the tokenizer’s configuration, vocabulary, merges (if applicable), and any other necessary files. This information define how text should be split into tokens and how these tokens map to the model's input ids in order for the tokenizer to function correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2478, 1037, 10938, 2121, 2897, 2003, 3722, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['using', 'a', 'transform', '##er', 'network', 'is', 'simple', '.']\n",
      "[2478, 1037, 10938, 2121, 2897, 2003, 3722, 1012]\n",
      "using a transformer network is simple.\n"
     ]
    }
   ],
   "source": [
    "sequence = \"Using a Transformer network is simple.\"\n",
    "res = tokenizer(sequence)\n",
    "print(res)\n",
    "# tokens gives tokens back\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "print(tokens)\n",
    "# ids gives the IDs of each token\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)\n",
    "# decoded_string goes the other way around and decodes the IDs to give the original sequence back.\n",
    "decoded_string = tokenizer.decode(ids)\n",
    "print(decoded_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attention mask in the 'res' dictionary is binary and is used to selectively focus on relavent parts of the input data.  When sending a batch into a transformer, the examples in the batch may have varying lengths.  Attention masks can pad the sequences so that all the examples in the batch have the same length.  The relavent part in a shorter example would be the sentence itself.  In the example above, all the tokens are given \"1\" so they are not padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the pipeline like before.\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data.\n",
    "X_train = [\"I've been waiting for a HuggingFace course my whole life.\", \"Python is great!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9598049521446228}, {'label': 'POSITIVE', 'score': 0.9998615980148315}]\n"
     ]
    }
   ],
   "source": [
    "res = classifier(X_train)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iferential statistics draws conclusions about populations based on a sample of the data.  Neural networks can give accurate predicitons based on the training data it is given, then tries to predict outcomes for new data sets.  Overfitting is when the model gives inaccurate predictions and has poor inference performance on new data.  The validation set is data not in the training set that is used as a measure on performance while training to test for overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the tokenizer to the X_train data, with padding to the longest sequence in the batch, (no padding for only a single sequence), \n",
    "# truncation to the 'max_length', (if max_length=None, this will truncate to the longest sequence of the data).\n",
    "# Set to return tensor parameters set to \"pt\" or PyTorch.\n",
    "batch = tokenizer(X_train, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "print(batch)\n",
    "\n",
    "# Since the model is not updating the parameters in the validation pass, the code can be sped up by turning off gradients using torch.no_grad():\n",
    "with torch.no_grad():\n",
    "    # Unpack the batch dictionary.\n",
    "    outputs = model(**batch)\n",
    "    print(outputs)\n",
    "    # Apply the softmax function to get the prediction probabilities.\n",
    "    predicitons = F.softmax(outputs.logits, dim=1)\n",
    "    print(predicitons)\n",
    "    # Get the labels at the probabilties which located at the maxima of the softmax function.\n",
    "    labels = torch.argmax(predicitons, dim=1)\n",
    "    print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
